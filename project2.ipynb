{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text\n",
       "0      20030219  aba decides against community broadcasting lic...\n",
       "1      20030219     act fire witnesses must be aware of defamation\n",
       "2      20030219     a g calls for infrastructure protection summit\n",
       "3      20030219           air nz staff in aust strike for pay rise\n",
       "4      20030219      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = pd.read_csv('/Users/camillecu/Downloads/KUL/text_mining/abcnews-date-text.csv')\n",
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1244184, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the following tasks:\n",
    "1. Apply the Latent Semantic Analysis and Latent Dirichlet Allocation technique to study the topic focus of ABC’s news headlines. Characterize them by exploring the most frequent words in each topic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/07/95/070d6e70f735d13f1c10afebb65ba3526125b7d6c6fc7022651a4a061148/scikit_learn-1.6.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading scikit_learn-1.6.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/camillecu/Downloads/KUL/text_mining/textmining_virtual/lib/python3.11/site-packages (from scikit-learn) (2.0.2)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Obtaining dependency information for scipy>=1.6.0 from https://files.pythonhosted.org/packages/b8/53/7f627c180cdaa211fa537650ca05912f58cb68fc33bb2f9af3d29169913e/scipy-1.15.0-cp311-cp311-macosx_10_13_x86_64.whl.metadata\n",
      "  Downloading scipy-1.15.0-cp311-cp311-macosx_10_13_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib>=1.2.0 in /Users/camillecu/Downloads/KUL/text_mining/textmining_virtual/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Obtaining dependency information for threadpoolctl>=3.1.0 from https://files.pythonhosted.org/packages/4b/2c/ffbf7a134b9ab11a67b0cf0726453cedd9c5043a4fe7a35d1cefa9a1bcfb/threadpoolctl-3.5.0-py3-none-any.whl.metadata\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.0-cp311-cp311-macosx_10_9_x86_64.whl (12.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.0-cp311-cp311-macosx_10_13_x86_64.whl (41.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
      "Successfully installed scikit-learn-1.6.0 scipy-1.15.0 threadpoolctl-3.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the data:\n",
    "Remove stopwords, punctuation, and special characters\n",
    "Convert text to lowercase\n",
    "Tokenize the headlines\n",
    "Apply lemmatization or stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/camillecu/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/camillecu/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/camillecu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/camillecu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')  # For lemmatization\n",
    "nltk.download('omw-1.4')  # Optional: For multilingual WordNet\n",
    "nltk.download('stopwords')  # For the stopwords list\n",
    "nltk.download('punkt')  # For word tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply the preprocess_text function to each row in the 'headline_text' column\n",
    "news['processed_text'] = news['headline_text'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a document-term matrix using TF-IDF vectorization is essential for several reasons:\n",
    "\n",
    "1. Numerical representation: It converts text data into a structured, numerical format suitable for machine learning algorithms[2]. This is crucial because most ML models require numerical inputs.\n",
    "\n",
    "2. Capturing word importance: TF-IDF scores reflect the importance of a word in a document relative to the entire corpus[1]. This helps in identifying key terms that are more relevant to each document.\n",
    "\n",
    "3. Balancing frequency and uniqueness: TF-IDF balances the term frequency (how often a word appears in a document) with its inverse document frequency (how rare or common a word is across all documents)[5]. This provides a more nuanced representation of the text data.\n",
    "\n",
    "4. Improved text analysis: By considering both the importance of terms within documents and across the corpus, TF-IDF allows for more effective text analysis in tasks such as classification, clustering, and information retrieval[2].\n",
    "\n",
    "5. Addressing common word bias: TF-IDF helps mitigate the issue of common words dominating the analysis by giving higher weight to terms that are specific to particular documents[1][5].\n",
    "\n",
    "6. Feature extraction: The resulting document-term matrix serves as a set of features that can be used for various NLP tasks, including document classification, sentiment analysis, and topic modeling[2][5].\n",
    "\n",
    "By creating a document-term matrix using TF-IDF vectorization, we transform raw text into a format that captures the semantic importance of words, enabling more accurate and meaningful analysis in natural language processing applications.\n",
    "\n",
    "Citations:\n",
    "[1] https://www.capitalone.com/tech/machine-learning/understanding-tf-idf/\n",
    "[2] https://app.studyraid.com/en/read/2545/51592/tf-idf-vectorization\n",
    "[3] https://hackernoon.com/document-term-matrix-in-nlp-count-and-tf-idf-scores-explained\n",
    "[4] https://openclassrooms.com/en/courses/6532301-introduction-to-natural-language-processing/8081363-apply-the-tf-idf-vectorization-approach\n",
    "[5] https://letsdatascience.com/tf-idf/\n",
    "[6] https://stackoverflow.com/questions/33510938/is-using-tf-idf-for-classification-task-like-sentiment-analysis-task-correct\n",
    "[7] https://www.learndatasci.com/glossary/tf-idf-term-frequency-inverse-document-frequency/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1244184, 5000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "# max_features=5000, which limits the number of features (unique words) to the top 5000 most frequent terms in the corpus. This helps in reducing the dimensionality of the feature space and can improve the performance of subsequent machine learning models.\n",
    "tfidf_matrix = vectorizer.fit_transform(news['processed_text'])\n",
    "tfidf_matrix.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LSA\n",
    "n_topics = 8\n",
    "lsa_model = TruncatedSVD(n_components=n_topics, random_state=42)\n",
    "lsa_topic_matrix = lsa_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Apply LDA\n",
    "lda_model = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "lda_topic_matrix = lda_model.fit_transform(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print top words for each topic\n",
    "def print_top_words(model, feature_names, n_top_words=10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        print(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA Topics:\n",
      "Topic 1: interview, police, man, new, extended, charged, court, murder, crash, death\n",
      "Topic 2: police, man, charged, new, court, murder, crash, death, woman, say\n",
      "Topic 3: man, charged, murder, police, court, missing, dy, jailed, stabbing, crash\n",
      "Topic 4: police, probe, investigate, officer, search, hunt, arrest, seek, driver, fatal\n",
      "Topic 5: new, police, man, zealand, year, case, york, murder, search, law\n",
      "Topic 6: say, court, face, man, govt, murder, new, plan, police, accused\n",
      "Topic 7: say, australia, crash, australian, win, day, world, woman, dy, car\n",
      "Topic 8: court, australia, face, win, charge, australian, murder, back, accused, day\n"
     ]
    }
   ],
   "source": [
    "# Print LSA topics\n",
    "print(\"LSA Topics:\")\n",
    "print_top_words(lsa_model, vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LDA Topics:\n",
      "Topic 1: test, black, future, election, murray, poll, broken, hill, go, candidate\n",
      "Topic 2: iraq, year, killed, kill, attack, blast, china, asylum, pakistan, protest\n",
      "Topic 3: police, man, charged, found, missing, court, charge, coast, murder, woman\n",
      "Topic 4: interview, cup, world, say, trump, medium, speaks, talk, john, closer\n",
      "Topic 5: market, rural, news, win, abc, record, country, share, day, weather\n",
      "Topic 6: council, plan, govt, health, water, call, new, hospital, service, centre\n",
      "Topic 7: crash, fire, death, rate, dy, car, man, road, victim, bushfire\n",
      "Topic 8: change, new, government, pay, say, wa, climate, govt, farmer, labor\n"
     ]
    }
   ],
   "source": [
    "# Print LDA topics\n",
    "print(\"\\nLDA Topics:\")\n",
    "print_top_words(lda_model, vectorizer.get_feature_names_out())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textmining_virtual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
